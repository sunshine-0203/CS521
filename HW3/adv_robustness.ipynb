{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whsg1XX_OZs6"
      },
      "source": [
        "# Set up for dataset and model\n",
        "\n",
        "Package installation, loading, and dataloaders. There's also a resnet18 model defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1domTvnONqD"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorboardX\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "# from tensorboardX import SummaryWriter\n",
        "\n",
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "batch_size = 64\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "# Dataloaders\n",
        "train_dataset = datasets.CIFAR10('cifar10_data/', train=True, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "test_dataset = datasets.CIFAR10('cifar10_data/', train=False, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def tp_relu(x, delta=1.):\n",
        "    ind1 = (x < -1. * delta).float()\n",
        "    ind2 = (x > delta).float()\n",
        "    return .5 * (x + delta) * (1 - ind1) * (1 - ind2) + x * ind2\n",
        "\n",
        "\n",
        "def tp_smoothed_relu(x, delta=1.):\n",
        "    ind1 = (x < -1. * delta).float()\n",
        "    ind2 = (x > delta).float()\n",
        "    return (x + delta) ** 2 / (4 * delta) * (1 - ind1) * (1 - ind2) + x * ind2\n",
        "\n",
        "\n",
        "class Normalize(nn.Module):\n",
        "    def __init__(self, mu, std):\n",
        "        super(Normalize, self).__init__()\n",
        "        self.mu, self.std = mu, std\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (x - self.mu) / self.std\n",
        "\n",
        "\n",
        "class IdentityLayer(nn.Module):\n",
        "    def forward(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "\n",
        "class PreActBlock(nn.Module):\n",
        "    '''Pre-activation version of the BasicBlock.'''\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, bn, learnable_bn, stride=1, activation='relu'):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.collect_preact = True\n",
        "        self.activation = activation\n",
        "        self.avg_preacts = []\n",
        "        self.bn1 = nn.BatchNorm2d(\n",
        "            in_planes, affine=learnable_bn) if bn else IdentityLayer()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=not learnable_bn)\n",
        "        self.bn2 = nn.BatchNorm2d(\n",
        "            planes, affine=learnable_bn) if bn else IdentityLayer()\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=not learnable_bn)\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=not learnable_bn)\n",
        "            )\n",
        "\n",
        "    def act_function(self, preact):\n",
        "        if self.activation == 'relu':\n",
        "            act = F.relu(preact)\n",
        "        elif self.activation[:6] == '3prelu':\n",
        "            act = tp_relu(preact, delta=float(\n",
        "                self.activation.split('relu')[1]))\n",
        "        elif self.activation[:8] == '3psmooth':\n",
        "            act = tp_smoothed_relu(preact, delta=float(\n",
        "                self.activation.split('smooth')[1]))\n",
        "        else:\n",
        "            assert self.activation[:8] == 'softplus'\n",
        "            beta = int(self.activation.split('softplus')[1])\n",
        "            act = F.softplus(preact, beta=beta)\n",
        "        return act\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.act_function(self.bn1(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(\n",
        "            self, 'shortcut') else x  # Important: using out instead of x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(self.act_function(self.bn2(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, n_cls, cuda=True, half_prec=False,\n",
        "                 activation='relu', fts_before_bn=False, normal='none'):\n",
        "        super(PreActResNet, self).__init__()\n",
        "        self.bn = True\n",
        "        self.learnable_bn = True  # doesn't matter if self.bn=False\n",
        "        self.in_planes = 64\n",
        "        self.avg_preact = None\n",
        "        self.activation = activation\n",
        "        self.fts_before_bn = fts_before_bn\n",
        "        if normal == 'cifar10':\n",
        "            self.mu = torch.tensor((0.4914, 0.4822, 0.4465)).view(1, 3, 1, 1)\n",
        "            self.std = torch.tensor((0.2471, 0.2435, 0.2616)).view(1, 3, 1, 1)\n",
        "        else:\n",
        "            self.mu = torch.tensor((0.0, 0.0, 0.0)).view(1, 3, 1, 1)\n",
        "            self.std = torch.tensor((1.0, 1.0, 1.0)).view(1, 3, 1, 1)\n",
        "            print('no input normalization')\n",
        "        if cuda:\n",
        "            self.mu = self.mu.cuda()\n",
        "            self.std = self.std.cuda()\n",
        "        if half_prec:\n",
        "            self.mu = self.mu.half()\n",
        "            self.std = self.std.half()\n",
        "\n",
        "        self.normalize = Normalize(self.mu, self.std)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=not self.learnable_bn)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.bn = nn.BatchNorm2d(512 * block.expansion)\n",
        "        self.linear = nn.Linear(512*block.expansion, n_cls)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, self.bn,\n",
        "                          self.learnable_bn, stride, self.activation))\n",
        "            # layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        for layer in [*self.layer1, *self.layer2, *self.layer3, *self.layer4]:\n",
        "            layer.avg_preacts = []\n",
        "\n",
        "        out = self.normalize(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        if return_features and self.fts_before_bn:\n",
        "            return out.view(out.size(0), -1)\n",
        "        out = F.relu(self.bn(out))\n",
        "        if return_features:\n",
        "            return out.view(out.size(0), -1)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def PreActResNet18(n_cls, cuda=True, half_prec=False, activation='relu', fts_before_bn=False,\n",
        "                   normal='none'):\n",
        "    # print('initializing PA RN-18 with act {}, normal {}'.format())\n",
        "    return PreActResNet(PreActBlock, [2, 2, 2, 2], n_cls=n_cls, cuda=cuda, half_prec=half_prec,\n",
        "                        activation=activation, fts_before_bn=fts_before_bn, normal=normal)\n",
        "\n",
        "\n",
        "# intialize the model\n",
        "model = PreActResNet18(10, cuda=True, activation='softplus1').to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCmWfZHTO8Oo"
      },
      "source": [
        "# Implement the Attacks\n",
        "\n",
        "Functions are given a simple useful signature that you can start with. Feel free to extend the signature as you see fit.\n",
        "\n",
        "You may find it useful to create a 'batched' version of PGD that you can use to create the adversarial attack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZjvA49yONqP"
      },
      "outputs": [],
      "source": [
        "def pgd_linf_untargeted(model, x, labels, k, eps, eps_step):\n",
        "    model.eval()\n",
        "    ce_loss = torch.nn.CrossEntropyLoss()\n",
        "    adv_x = x.clone().detach()\n",
        "    adv_x.requires_grad_(True)\n",
        "    for _ in range(k):\n",
        "        adv_x.requires_grad_(True)\n",
        "        model.zero_grad()\n",
        "        output = model(adv_x)\n",
        "        # TODO: Calculate the loss\n",
        "        loss = ce_loss(output, labels)\n",
        "        loss.backward()\n",
        "        # TODO: compute the adv_x\n",
        "        # find delta, clamp with eps\n",
        "        with torch.no_grad():\n",
        "            grad_sign = adv_x.grad.sign()\n",
        "            adv_x = adv_x + eps_step * grad_sign\n",
        "            delta = torch.clamp(adv_x - x, min=-eps, max=eps)\n",
        "            adv_x = x + delta\n",
        "            adv_x = torch.clamp(adv_x, 0.0, 1.0)\n",
        "\n",
        "    return adv_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pgd_l2_untargeted(model, x, labels, k, eps, eps_step):\n",
        "    model.eval()\n",
        "    ce_loss = torch.nn.CrossEntropyLoss()\n",
        "    adv_x = x.clone().detach()\n",
        "    adv_x.requires_grad_(True)\n",
        "    for _ in range(k):\n",
        "        adv_x.requires_grad_(True)\n",
        "        model.zero_grad()\n",
        "        output = model(adv_x)\n",
        "        batch_size = x.size()[0]\n",
        "        # TODO: Calculate the loss\n",
        "        loss = ce_loss(output, labels)\n",
        "        loss.backward()\n",
        "        grad = adv_x.grad.data\n",
        "        # TODO: compute the adv_x\n",
        "        # find delta, clamp with eps, project delta to the l2 ball\n",
        "        # HINT: https://github.com/Harry24k/adversarial-attacks-pytorch/blob/master/torchattacks/attacks/pgdl2.py\n",
        "        with torch.no_grad():\n",
        "            grad = adv_x.grad\n",
        "\n",
        "            # normalize gradient in L2\n",
        "            grad_norm = grad.view(grad.size(0), -1).norm(p=2, dim=1)\n",
        "            grad_norm = grad_norm.view(-1, *([1] * (grad.dim() - 1))) + 1e-10\n",
        "            \n",
        "            grad_unit = grad / grad_norm\n",
        "\n",
        "            # take a step\n",
        "            adv_x = adv_x + eps_step * grad_unit\n",
        "\n",
        "            # project back to L2 ball\n",
        "            delta = adv_x - x\n",
        "            delta_norm = delta.view(delta.size(0), -1).norm(p=2, dim=1)\n",
        "            delta_norm = delta_norm.view(-1, *\n",
        "                                         ([1] * (delta.dim() - 1))) + 1e-10\n",
        "            delta_norm_clip = torch.clamp(delta_norm, min=eps) / eps   # =1 if ||delta|| <= eps, >1 otherwise\n",
        "            delta = delta / delta_norm_clip\n",
        "\n",
        "            adv_x = x + delta\n",
        "            adv_x = torch.clamp(adv_x, 0.0, 1.0)\n",
        "\n",
        "    return adv_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mja_AB4RykO"
      },
      "source": [
        "# Evaluate Single and Multi-Norm Robust Accuracy\n",
        "\n",
        "In this section, we evaluate the model on the Linf and L2 attacks as well as union accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model_on_single_attack(model, attack='pgd_linf', eps=0.1):\n",
        "    model.eval()\n",
        "    tot_test, tot_acc, tot_clean = 0.0, 0.0, 0.0\n",
        "    for batch_idx, (x_batch, y_batch) in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Evaluating\"):\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        \n",
        "        if attack == 'pgd_linf':\n",
        "            # TODO: get x_adv untargeted pgd linf with eps, and eps_step=eps/4\n",
        "            x_adv = pgd_linf_untargeted(model, x_batch, y_batch,\n",
        "                                        k=10,\n",
        "                                        eps=eps,\n",
        "                                        eps_step=eps/4)\n",
        "        elif attack == 'pgd_l2':\n",
        "            # TODO: get x_adv untargeted pgd l2 with eps, and eps_step=eps/4\n",
        "            x_adv = pgd_l2_untargeted(model, x_batch, y_batch,\n",
        "                                      k=10,\n",
        "                                      eps=eps,\n",
        "                                      eps_step=eps/4)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        # get the testing accuracy and update tot_test and tot_acc\n",
        "        with torch.no_grad():\n",
        "            out = model(x_adv)\n",
        "            out_clean = model(x_batch)\n",
        "            preds = out.argmax(dim=1)\n",
        "            preds_clean = out_clean.argmax(dim=1)\n",
        "            correct = (preds == y_batch).sum().item()\n",
        "            correct_clean = (preds_clean == y_batch).sum().item()\n",
        "        tot_clean += correct_clean\n",
        "        tot_acc += correct\n",
        "        tot_test += x_batch.size(0)\n",
        "\n",
        "    print('Standard accuracy %.5lf' % (tot_clean/tot_test))\n",
        "    print('Robust accuracy %.5lf' % (tot_acc/tot_test),\n",
        "          f'on {attack} attack with eps = {eps}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PGD Adversarial Training & Evaluation (added)\n",
        "This section was appended automatically to run PGD adversarial training, compare with standard training, report accuracies across epsilons, and visualize adversarial examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fgsm_attack(model, x, y, eps):\n",
        "    return pgd_linf_untargeted(model, x, y, k=1, eps=eps, eps_step=eps, random_start=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def eval_clean_accuracy(model, loader):\n",
        "    model.eval()\n",
        "    total=0; correct=0\n",
        "    for xb,yb in loader:\n",
        "        xb,yb=xb.to(device), yb.to(device)\n",
        "        out=model(xb)\n",
        "        pred=out.argmax(1)\n",
        "        total+=xb.size(0); correct+=(pred==yb).sum().item()\n",
        "    return 100.0*correct/total\n",
        "\n",
        "def eval_robust_accuracy(model, loader, eps=0.1, steps=20):\n",
        "    model.eval()\n",
        "    total=0; correct=0\n",
        "    for xb,yb in loader:\n",
        "        xb,yb=xb.to(device), yb.to(device)\n",
        "        xb_adv = pgd_linf_untargeted(model, xb, yb, k=steps, eps=eps, eps_step=eps/4.0, random_start=True)\n",
        "        with torch.no_grad():\n",
        "            out = model(xb_adv)\n",
        "            pred = out.argmax(1)\n",
        "            total += xb.size(0)\n",
        "            correct += (pred==yb).sum().item()\n",
        "    return 100.0*correct/total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training helpers: standard and adversarial (on-the-fly PGD)\n",
        "def standard_train(model, loader, epochs=3, lr=1e-3):\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    start=time.time()\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        running=0.0\n",
        "        for xb,yb in tqdm(loader, desc=f\"Std ep {ep+1}/{epochs}\"):\n",
        "            xb,yb=xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            out=model(xb)\n",
        "            loss=ce(out,yb)\n",
        "            loss.backward(); opt.step()\n",
        "            running+=loss.item()\n",
        "        print(f\"Std epoch {ep+1} avg_loss={running/len(loader):.4f}\")\n",
        "    return time.time()-start\n",
        "\n",
        "def adversarial_train_pgd(model, loader, epochs=3, eps=8/255, k=10, lr=1e-3):\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    start=time.time()\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        running=0.0\n",
        "        for xb,yb in tqdm(loader, desc=f\"Adv ep {ep+1}/{epochs}\"):\n",
        "            xb,yb=xb.to(device), yb.to(device)\n",
        "            xb_adv = pgd_linf_untargeted(model, xb, yb, k=k, eps=eps, eps_step=eps/4.0, random_start=True)\n",
        "            opt.zero_grad()\n",
        "            out_adv = model(xb_adv)\n",
        "            out_clean = model(xb)\n",
        "            loss = 0.5 * ce(out_clean, yb) + 0.5 * ce(out_adv, yb)\n",
        "            loss.backward(); opt.step()\n",
        "            running+=loss.item()\n",
        "        print(f\"Adv epoch {ep+1} avg_loss={running/len(loader):.4f}\")\n",
        "    return time.time()-start\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run experiments over epsilons 0.01..0.1 (10 values). Uses train_loader/test_loader defined earlier in your notebook.\n",
        "eps_values = [2/255, 8/255]\n",
        "results=[]\n",
        "std_epochs=10\n",
        "adv_epochs=10\n",
        "\n",
        "for eps in eps_values:\n",
        "    print('\\n'+'='*50)\n",
        "    print(f\"Running for eps={eps:.3f}\")\n",
        "    m_std = PreActResNet18(10, cuda=True, activation='softplus1').to(device)\n",
        "    m_adv = PreActResNet18(10, cuda=True, activation='softplus1').to(device)\n",
        "\n",
        "    # standard train\n",
        "    t_std = standard_train(m_std, train_loader, epochs=std_epochs)\n",
        "    clean_std = eval_clean_accuracy(m_std, test_loader)\n",
        "    rob_std = eval_robust_accuracy(m_std, test_loader, eps=eps, steps=20)\n",
        "    print(f\"Standard: clean={clean_std:.2f}%, PGD({eps:.3f})={rob_std:.2f}%, time={t_std:.1f}s\")\n",
        "\n",
        "    # adversarial train\n",
        "    t_adv = adversarial_train_pgd(m_adv, train_loader, epochs=adv_epochs, eps=eps, k=10)\n",
        "    clean_adv = eval_clean_accuracy(m_adv, test_loader)\n",
        "    rob_adv = eval_robust_accuracy(m_adv, test_loader, eps=eps, steps=20)\n",
        "    print(f\"Adversarial: clean={clean_adv:.2f}%, PGD({eps:.3f})={rob_adv:.2f}%, time={t_adv:.1f}s\")\n",
        "\n",
        "    fgsm_std = eval_robust_accuracy(m_std, test_loader, eps=eps, steps=1)\n",
        "    fgsm_adv = eval_robust_accuracy(m_adv, test_loader, eps=eps, steps=1)\n",
        "    print(f\"FGSM(k=1) acc: std={fgsm_std:.2f}%, adv={fgsm_adv:.2f}%\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "print('\\n=== Summary ===')\n",
        "display(df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
